{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing Summary:\n",
    "\n",
    "This notebook will classify wine type and predict the wine rating using the Machine Learning algorithms. This Notebook is pretty basic and simple :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df = pd.read_csv('/home/docode/project/EDA on Collected Data/new_wine.csv', index_col = 0) \n",
    "\n",
    "# One Hot Encoding through pandas get dummies\n",
    "categorical_features_to_encode = ['wine region', 'wine country', 'grape information'] \n",
    "\n",
    "wine_df = pd.get_dummies(wine_df, columns = categorical_features_to_encode, prefix = categorical_features_to_encode)\n",
    "\n",
    "# Some wines have wine year as 'N.V.', need to convert them to NaN\n",
    "wine_df['wine year'] = wine_df['wine year'].apply(lambda x: np.nan if x == 'N.V.' else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the X and Y variables\n",
    "X = wine_df.iloc[:, 2:].drop(columns = ['wine type', 'wine description'])\n",
    "X = X.fillna(0.0)\n",
    "Y = wine_df['wine type']\n",
    "Y = Y.fillna(0.0)\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_accuracy 49.207505920932775 %:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr_model=LogisticRegression(random_state=0)\n",
    "lr_model.fit(X_train,y_train)\n",
    "lr_pred=lr_model.predict(X_test)\n",
    "lr_cm=confusion_matrix(y_test,lr_pred)\n",
    "lr_ac=accuracy_score(y_test, lr_pred)\n",
    "print('LogisticRegression_accuracy {} %:'.format(lr_ac * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier accuracy: 93.35033703771178 %\n"
     ]
    }
   ],
   "source": [
    "dtree_model=DecisionTreeClassifier(criterion='entropy',random_state=0)\n",
    "dtree_model.fit(X_train,y_train)\n",
    "dtree_pred=dtree_model.predict(X_test)\n",
    "dtree_cm=confusion_matrix(y_test,dtree_pred)\n",
    "dtree_ac=accuracy_score(dtree_pred,y_test)\n",
    "print('Decision Tree Classifier accuracy: {} %'.format(dtree_ac * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest accuracy: 94.69848788486063 %\n"
     ]
    }
   ],
   "source": [
    "rdf_model=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)\n",
    "rdf_model.fit(X_train,y_train)\n",
    "rdf_pred=rdf_model.predict(X_test)\n",
    "rdf_cm=confusion_matrix(y_test,rdf_pred)\n",
    "rdf_ac=accuracy_score(rdf_pred,y_test)\n",
    "print('Random Forest accuracy: {} %'.format(rdf_ac * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Results using Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross - validation scores: [0.95104736 0.94579822 0.95103621 0.94124345 0.95445229]\n",
      "Mean score of all folds: 94.94442494770101 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "kfold = KFold(n_splits = 5, shuffle = True) # Setting shuffle as True considerably increases Cross Validation Score\n",
    "\n",
    "print('Cross - validation scores:', cross_val_score(rdf_model, X, Y, cv = kfold))\n",
    "print('Mean score of all folds: {} %'.format (cross_val_score(rdf_model, X, Y, cv = kfold).mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Can See that the Random Forest mean accuracy is very similar to cross validation. \n",
    "\n",
    "Therefore, We can conclude that our model is very accurate in classifying the type of wine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Rating Prediction\n",
    "Now we will try to predict wine rating using the ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df = pd.read_csv('/home/docode/project/EDA on Collected Data/new_wine.csv', index_col = 0) \n",
    "wine_df\n",
    "\n",
    "# One Hot Encoding through pandas get dummies\n",
    "categorical_features_to_encode = ['wine type','wine region', 'wine country', 'grape information'] \n",
    "\n",
    "wine_df = pd.get_dummies(wine_df, columns = categorical_features_to_encode, prefix = categorical_features_to_encode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and Y variables\n",
    "X = wine_df.iloc[:, 2:].drop(columns = ['wine rating', 'wine description'])\n",
    "X = X.fillna(0.0)\n",
    "Y = wine_df['wine rating']\n",
    "Y = Y.fillna(0.0)\n",
    "\n",
    "# Doing Train, test and split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.5400526398714944\n",
      "RMSE: 0.23928295142676836\n",
      "MAE: 0.17691747130624885\n"
     ]
    }
   ],
   "source": [
    "dtree_model=DecisionTreeRegressor()\n",
    "dtree_model.fit(X_train,y_train)\n",
    "dtree_pred=dtree_model.predict(X_test)\n",
    "\n",
    "print('R2 score:', r2_score( y_test, dtree_pred))\n",
    "\n",
    "print('RMSE:', mean_squared_error(y_test, dtree_pred, squared=False))\n",
    "\n",
    "print('MAE:', mean_absolute_error(y_test, dtree_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raindom Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.7353305448367043\n",
      "RMSE: 0.1815137809365554\n",
      "MAE: 0.13688018049085174\n"
     ]
    }
   ],
   "source": [
    "rfg_model = RandomForestRegressor(n_estimators=200, random_state = 42) # n_estimators=10,criterion='entropy',random_state=0\n",
    "rfg_model.fit(X_train, y_train)\n",
    "rfg_pred = rfg_model.predict(X_test)\n",
    "\n",
    "print('R2 score:', r2_score( y_test, rfg_pred))\n",
    "\n",
    "print('RMSE:', mean_squared_error(y_test, rfg_pred, squared=False))\n",
    "\n",
    "print('MAE:', mean_absolute_error(y_test, rfg_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.6896991507081625\n",
      "RMSE: 0.1965391953545685\n",
      "MAE: 0.1512078702860266\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsRegressor(n_neighbors = 10)\n",
    "knn_model.fit(X_train, y_train)\n",
    "knn_pred = knn_model.predict(X_test)\n",
    "\n",
    "print('R2 score:', r2_score(y_test, knn_pred))\n",
    "\n",
    "print('RMSE:', mean_squared_error(y_test, knn_pred, squared=False))\n",
    "\n",
    "print('MAE:', mean_absolute_error(y_test, knn_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "The Best Wine Rating (Quality) prediction and wine classification predictions come from Random Forest Algorithm. The model had the best score both in classification and regression problems. To get better results One Hot Encoding were used. Without Encoding the dataframe, the accuracy of the model were smaller for 6-8%.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of the Notebook and Project:\n",
    "This marks the end of the Notebook and Project!\n",
    "\n",
    "Overall I have split the project into 6 different notebooks:\n",
    "1) Wine Data Collection from \n",
    "2) Exploratory Data Analysis on Collected Data\n",
    "3) Constructing User Rating Dataset for Recommendation System\n",
    "4) Building a Recommendation System\n",
    "5) Description Based Recommendation System\n",
    "6) Wine Classification and Wine Quality Prediction\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
